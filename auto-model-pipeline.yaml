
parameters:
 - name: model_name
   displayName: mode name on huggingface website
   type: string
   default: meta-llama/Llama-3.2-3B-Instruct

pool: onnxruntime-Linux-GPU-T4 #onnxruntime-tensorrt-linuxbuild-T4
steps:
 - task: UsePythonVersion@0
   inputs:
     versionSpec: '3.12'
     architecture: 'x64'

#  - bash: |
#      pip install -r llama-requirements.txt
#    displayName: install the necessary packages
#    workingDirectory: $(Build.SourcesDirectory)


#  - script: az --version
#    displayName: 'Show Azure CLI version'

#  - script: |
#       python -c "with open('hello.txt', 'w') as f:f.write('hello world')"
#       azcopy cp --recursive "./hello.txt" 'https://sunghchostorageaccount.blob.core.windows.net/test'
#       ls
#    displayName: 'try azcopy upload to sun blob'
 - bash: |
    nvidia-smi
   displayName: "dump nvidia-smi"
 
#  - bash: |
#     ls -l /usr
#     nvcc --version
#    displayName: "dump cuda version"

#  - script: |
#      mkdir output_model
#      azcopy cp --recursive "https://sunghchostorageaccount.blob.core.windows.net/phi-3/phi-3.5-mini-instruct/onnx/cuda/cuda-int4-rtn-block-32" 'output_model/phi-3.5-mini-instruct-cuda-int4'
#      ls -l output_model/phi-3.5-mini-instruct-cuda-int4/cuda-int4-rtn-block-32
#      azcopy cp --recursive "https://sunghchostorageaccount.blob.core.windows.net/phi-3/phi-3.5-mini-instruct/hf_version" 'output_model/hf_version'
#      ls -l output_model/hf_version
#    displayName: "download model from blob"
#    workingDirectory: $(Build.BinariesDirectory)

#  - script: |
#      # search for "genai_config.json" under folder "$(Build.BinariesDirectory)/output_model/"
#      model_config_folder=`find $(Build.BinariesDirectory)/output_model/ -name genai_config.json -printf "%h\n"`
#      baseline_model_folder=`find $(Build.BinariesDirectory)/output_model/ -name config.json -printf "%h\n"`

#      # verify only one config is found
#      num_models=`echo -n "$model_config_folder" | grep -c '^'`
#      if [[ $num_models -ne 1 ]]; then
#        echo "The output model folder should contains exactly one genai_config.json file, but found $num_models"
#        exit 1
#      fi

#      echo "model folder: $model_config_folder"
#      echo "baseline_model folder: $baseline_model_folder"

#      docker run --gpus all --rm \
#         --ipc=host \
#         --volume $(Build.SourcesDirectory):/ort_src \
#         --volume $(Build.BinariesDirectory):/build \
#         --volume $model_config_folder:/model \
#         --volume $baseline_model_folder:/baseline \
#         -p 8000:8000 \
#         -e CCACHE_DIR=/cache -w /ort_src \
#         ptebic.azurecr.io/public/aifx/acpt/stable-ubuntu2004-cu121-py310-torch222:biweekly.202410.2 /bin/bash /ort_src/docker_script.sh
#    workingDirectory: $(Build.SourcesDirectory)
#    displayName: "start model endpoint and RAI eval in container"


#  - script: |
#      pip install fastapi
#      pip install uvicorn
#      pip install onnxruntime-genai-cuda --upgrade
#      uvicorn main:app --reload
#    displayName: "start model endpoint"
#    workingDirectory: $(Build.SourcesDirectory)

#  - script: |
#      pip install azure-ai-evaluation --upgrade
#      pip install promptflow-azure --upgrade
#      python ./evaluate-models-target-IP.py
#    displayName: "run RAI script"
#    workingDirectory: $(Build.SourcesDirectory)

#  - bash: |
#      echo Authenticate with Huggingface repository 

#      huggingface-cli login --token $(hf_token)

#    displayName: "Authentication to Huggingface repo" 

 - script: |
     docker run --gpus all --rm \
        --ipc=host \
        --volume $(Build.SourcesDirectory):/ort_src \
        --volume $(Build.BinariesDirectory):/build \
        -e CCACHE_DIR=/cache -w /ort_src \
        -e HF_TOKEN=$(hf_token) \
        ptebic.azurecr.io/public/aifx/acpt/stable-ubuntu2004-cu121-py310-torch222:biweekly.202410.2 /bin/bash /ort_src/docker_export_model_script.sh
   workingDirectory: $(Build.SourcesDirectory)
   displayName: "Export Onnx model by turnkey"


 - script: |
     # search for "genai_config.json" under folder "$(Build.BinariesDirectory)/oga_models/"
     model_config_folder=`find $(Build.BinariesDirectory)/oga_models/ -name genai_config.json -printf "%h\n"`
     

     # verify only one config is found
     num_models=`echo -n "$model_config_folder" | grep -c '^'`
     if [[ $num_models -ne 1 ]]; then
       echo "The output model folder should contains exactly one genai_config.json file, but found $num_models"
       exit 1
     fi

     echo "model folder: $model_config_folder"
     

     docker run --gpus all --rm \
        --ipc=host \
        --volume $(Build.SourcesDirectory):/ort_src \
        --volume $(Build.BinariesDirectory):/build \
        --volume $model_config_folder:/model \
        -p 8000:8000 \
        -e CCACHE_DIR=/cache -w /ort_src \
        ptebic.azurecr.io/public/aifx/acpt/stable-ubuntu2004-cu121-py310-torch222:biweekly.202410.2 /bin/bash /ort_src/docker_script.sh
   workingDirectory: $(Build.SourcesDirectory)
   displayName: "start model endpoint and RAI eval in container"

#baseline_model_folder=`find $(Build.BinariesDirectory)/output_model/ -name config.json -printf "%h\n"`
#echo "baseline_model folder: $baseline_model_folder"
#--volume $baseline_model_folder:/baseline \



#  - script: | 

#      python $(Build.SourcesDirectory)/llama_model_builder.py --model_name '${{ parameters.model_name }}' --output_dir '$(Build.BinariesDirectory)'

#    displayName: "Convert Huggingface model to ONNX model"
     
#    workingDirectory: $(Build.SourcesDirectory)

 - task: AzureCLI@2
   displayName: 'upload model to Blob Storage'
   inputs:
     azureSubscription: AIInfraBuild
     scriptLocation: inlineScript
     scriptType: bash
     inlineScript: |
       cd $(Build.BinariesDirectory)
       azcopy copy './oga_models' 'https://sunghchostorageaccount.blob.core.windows.net/test' --recursive


#  - task: AzureCLI@2
#    displayName: 'upload model to AzureML registry'
#    inputs:
#      azureSubscription: AIInfraBuild
#      scriptLocation: inlineScript
#      scriptType: bash
#      inlineScript: |
#        cd $(Build.BinariesDirectory)
#        az extension add -n ml -y
#        az extension list
#        az account list
#        az ml data create --name llama32-3b-instruct-for-test-only --version 1 --path './models/${{ parameters.model_name }}/output_model/model' --registry-name model-publish-test --description "llama3.2-3b-instruct-test-only from pipeline"


#- bash: |
 #    pip install fastapi
 #    pip install uvicorn
 #    pip install onnxruntime-genai-cuda

 #  displayName: install fastapi packages

 #- script: | 

 #    uvicorn main:app --reload

 #  displayName: "Start model endpoint"
     
 #  workingDirectory: $(Build.SourcesDirectory)
