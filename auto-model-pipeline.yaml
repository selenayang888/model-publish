
parameters:
 - name: model_name
   displayName: mode name on huggingface website
   type: string
   default: meta-llama/Llama-3.2-3B-Instruct

pool: onnxruntime-tensorrt-linuxbuild-T4 #onnxruntime-Linux-GPU-T4
steps:
 - task: UsePythonVersion@0
   inputs:
     versionSpec: '3.12'
     architecture: 'x64'

#  - bash: |
#      pip install -r llama-requirements.txt
#    displayName: install the necessary packages
#    workingDirectory: $(Build.SourcesDirectory)


#  - script: az --version
#    displayName: 'Show Azure CLI version'

#  - script: |
#       python -c "with open('hello.txt', 'w') as f:f.write('hello world')"
#       azcopy cp --recursive "./hello.txt" 'https://sunghchostorageaccount.blob.core.windows.net/test'
#       ls
#    displayName: 'try azcopy upload to sun blob'
 - bash: |
    nvidia-smi
   displayName: "dump nvidia-smi"

 - script: |
     azcopy cp --recursive "https://sunghchostorageaccount.blob.core.windows.net/phi-3/phi-3.5-mini-instruct/onnx/cuda/cuda-int4-rtn-block-32" './phi-3.5-mini-instruct-cuda-int4'
   displayName: "download model from blob"
   workingDirectory: $(Build.SourcesDirectory)

 - script: |
     pip install fastapi
     pip install uvicorn
     pip install onnxruntime-genai-cuda --upgrade
     apt-get install libcublas-12-0
     uvicorn main:app --reload
   displayName: "start model endpoint"
   workingDirectory: $(Build.SourcesDirectory)

 - script: |
     pip install azure-ai-evaluation --upgrade
     pip install promptflow-azure --upgrade
     python ./evaluate-models-target-IP.py
   displayName: "run RAI script"
   workingDirectory: $(Build.SourcesDirectory)

 - bash: |
     echo Authenticate with Huggingface repository 

     huggingface-cli login --token $(hf_token)

   displayName: "Authentication to Huggingface repo" 

 - bash: |
    pip list 
   displayName: "dump pip list"



#  - script: | 

#      python $(Build.SourcesDirectory)/llama_model_builder.py --model_name '${{ parameters.model_name }}' --output_dir '$(Build.BinariesDirectory)'

#    displayName: "Convert Huggingface model to ONNX model"
     
#    workingDirectory: $(Build.SourcesDirectory)

#  - task: AzureCLI@2
#    displayName: 'upload model to Blob Storage'
#    inputs:
#      azureSubscription: AIInfraBuild
#      scriptLocation: inlineScript
#      scriptType: bash
#      inlineScript: |
#        cd $(Build.BinariesDirectory)
#        azcopy copy './models/${{ parameters.model_name }}/output_model/model' 'https://sunghchostorageaccount.blob.core.windows.net/test' --recursive


#  - task: AzureCLI@2
#    displayName: 'upload model to AzureML registry'
#    inputs:
#      azureSubscription: AIInfraBuild
#      scriptLocation: inlineScript
#      scriptType: bash
#      inlineScript: |
#        cd $(Build.BinariesDirectory)
#        az extension add -n ml -y
#        az extension list
#        az account list
#        az ml data create --name llama32-3b-instruct-for-test-only --version 1 --path './models/${{ parameters.model_name }}/output_model/model' --registry-name model-publish-test --description "llama3.2-3b-instruct-test-only from pipeline"


#- bash: |
 #    pip install fastapi
 #    pip install uvicorn
 #    pip install onnxruntime-genai-cuda

 #  displayName: install fastapi packages

 #- script: | 

 #    uvicorn main:app --reload

 #  displayName: "Start model endpoint"
     
 #  workingDirectory: $(Build.SourcesDirectory)
