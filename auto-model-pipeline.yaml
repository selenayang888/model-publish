trigger:
 - main 

schedules: 

 - cron: "0 0 * * *" 

   displayName: Daily midnight build 

   branches: 

     include: 
       - main 

   always: true 

parameters:
 - name: model_name
   displayName: mode name on huggingface website
   type: string
   default: meta-llama/Llama-3.2-3B-Instruct

pool: onnxruntime-Linux-GPU-T4
steps:
 - task: UsePythonVersion@0
   inputs:
     versionSpec: '3.10'
     architecture: 'x64'

 - bash: |
     pip install -r llama-requirements.txt

   displayName: install the necessary packages
   workingDirectory: $(Build.SourcesDirectory)

 - script: pip install --pre azure-cli --extra-index-url https://azurecliprod.blob.core.windows.net/edge
   displayName: 'upgrade azure cli'

 - script: az --version
   displayName: 'Show Azure CLI version'

 - task: AzureCLI@2
   displayName: 'diagnosis azcli 1'
   inputs:
     azureSubscription: AIInfraBuild
     scriptLocation: inlineScript
     scriptType: bash
     inlineScript: |
       which az
       az --version
       az extension list


 - script: |
     az extension add -n ml -y
   displayName: "az install ml extension"

 - bash: |
     echo Authenticate with Huggingface repository 

     huggingface-cli login --token $(hf_token)

   displayName: "Authentication to Huggingface repo" 

 - bash: |
    pip list 
   displayName: "dump pip list"

 #- bash: |
 #   nvidia-smi
 #  displayName: "dump nvidia-smi"

 - script: | 

     python $(Build.SourcesDirectory)/llama_model_builder.py --model_name '${{ parameters.model_name }}' --output_dir '$(Build.SourcesDirectory)'

   displayName: "Convert Huggingface model to ONNX model"
     
   workingDirectory: $(Build.SourcesDirectory)


 - task: AzureCLI@2
   displayName: 'upload model to AzureML registry'
   inputs:
     azureSubscription: AIInfraBuild
     scriptLocation: inlineScript
     scriptType: bash
     inlineScript: |
       cd $(Build.BinariesDirectory)
       az extension add -n ml -y
       az ml data create --name llama32-3b-instruct-for-test-only --version 1 --path './models/${{ parameters.model_name }}/output_model/model' --registry-name Genai-model-registry --description "llama3.2-3b-instruct-test-only from pipeline"

#- bash: |
 #    pip install fastapi
 #    pip install uvicorn
 #    pip install onnxruntime-genai-cuda

 #  displayName: install fastapi packages

 #- script: | 

 #    uvicorn main:app --reload

 #  displayName: "Start model endpoint"
     
 #  workingDirectory: $(Build.SourcesDirectory)
